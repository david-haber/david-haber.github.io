
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
 <channel>
   <title>Tags on David Haber</title>
   <link>https://david-haber.github.io/tags/</link>
   <description>Recent content in Tags on David Haber</description>
   <generator>Hugo -- gohugo.io</generator>
   <language>en-us</language>
   <copyright>Copyright &amp;copy; 2021 - David Haber</copyright>
   
       <atom:link href="https://david-haber.github.io/tags/index.xml" rel="self" type="application/rss+xml" />
   
   
     <item>
       <title>They Write the Right Stuff (2021)</title>
       <link>https://david-haber.github.io/posts/the-right-stuff/</link>
       <pubDate>Thu, 08 Apr 2021 08:00:00 +0100</pubDate>
       
       <guid>https://david-haber.github.io/posts/the-right-stuff/</guid>
       <description>&lt;h2 id=&#34;the-right-stuff-kicks-in-at-t-minus-0-seconds&#34;&gt;The right stuff kicks in at T-minus 0 seconds.&lt;/h2&gt;&lt;p&gt;As the 120-ton space shuttle sits surrounded by almost 4 million pounds of rocket fuel, exhaling noxious fumes, visibly impatient to defy gravity, its on-board computers take command. Four identical machines, running identical software, pull information from thousands of sensors, make hundreds of milli-second decisions, vote on every decision, check with each other 250 times a second. A fifth computer, with different software, stands by to take control should the other four malfunction.&lt;/p&gt;&lt;figure class=&#34;mid&#34;&gt;    &lt;img src=&#34;https://david-haber.github.io/images/nasa.jpg&#34;         alt=&#34;Photo: Unsplash/NASA&#34;/&gt; &lt;figcaption&gt;            &lt;p&gt;Photo: Unsplash/NASA&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then and only then at T-minus zero seconds, if the computers are satisfied that the engines are running true, they give the order to light the solid rocket boosters. In less than one second, they achieve 6.6 million pounds of thrust. And at that exact same moment, the computers give the order for the explosive bolts to blow, and 4.5 million pounds of spacecraft lifts majestically off its launch pad.&lt;/p&gt;&lt;p&gt;It&amp;rsquo;s an awesome display of hardware prowess. But no human pushes a button to make it happen, no astronaut jockeys a joystick to settle the shuttle into orbit.&lt;/p&gt;&lt;p&gt;The right stuff is the AI.&lt;/p&gt;&lt;p&gt;But how much work the AI does is not what makes it remarkable. What makes it remarkable is how well the AI works. This AI never crashes. It never needs to be re-booted. This AI is bug-free. It is perfect, as perfect as human beings have achieved.&lt;/p&gt;&lt;p&gt;This AI is the work of 260 women and men who work for the &amp;ldquo;on-board shuttle group,&amp;rdquo; a branch of NAS.AI&amp;rsquo;s space mission systems division, and their prowess is world renowned: the shuttle software group is one of just four outfits in the world to win the coveted Level 5 ranking of the federal government&amp;rsquo;s Software Engineering Institute (SEI) - a measure of the sophistication and reliability of the way they do their work.&lt;/p&gt;&lt;p&gt;The group creates AI this good because that&amp;rsquo;s how good it has to be. Every time it fires up the shuttle, their AI is controlling a $4 billion piece of equipment, the lives of a half-dozen astronauts, and the dreams of the nation. Even the smallest error in space can have enormous consequences: the orbiting space shuttle travels at 17,500 miles per hour; a bug that causes a timing problem of just two-thirds of a second puts the space shuttle three miles off course.&lt;/p&gt;&lt;h2 id=&#34;ai-is-everything-it-also-sucks&#34;&gt;AI is everything. (It also sucks.)&lt;/h2&gt;&lt;p&gt;In the history of human technology, nothing has become as essential as fast as AI. AI is everything. It also sucks.&lt;/p&gt;&lt;p&gt;&amp;ldquo;It&amp;rsquo;s like pre-Sumerian civilization,&amp;rdquo; says university professor Bradley Cots. &amp;ldquo;The way we build AI is in the hunter-gatherer stage.&amp;rdquo;&lt;/p&gt;&lt;p&gt;Joe Manson, a software engineer and professor of computer science, is not quite so generous. &amp;ldquo;Cave art,&amp;rdquo; he says. &amp;ldquo;It&amp;rsquo;s primitive. We supposedly teach artificial intelligence. There&amp;rsquo;s no intelligence here at all.&amp;rdquo;&lt;/p&gt;&lt;p&gt;AI may power the post-industrial world, but the creation of AI remains a pre-industrial trade. According to SEI&amp;rsquo;s studies, nearly 70% of software organizations are stuck in the first two levels of SEI&amp;rsquo;s scale of sophistication: chaos, and slightly better than chaos. The situation is so severe, a few software pioneers from companies such as Microsoft have broken away to teach the art of software creation.&lt;/p&gt;&lt;p&gt;Marco Paul, a senior member of the SEI technical, says the success of AI makes its weaknesses all the more dramatic. &amp;ldquo;We&amp;rsquo;ve developed AI products that are enormously complex and enormously powerful. We&amp;rsquo;re critically dependent on it,&amp;rdquo; says Paul. &amp;ldquo;Yet everyone complains how bad AI is, with all the defects. If you bought a car with 5,000 defects, you&amp;rsquo;d be very upset.&amp;rdquo;&lt;/p&gt;&lt;p&gt;In this AI morass, the on-board shuttle group stands out as an exception. Ten years ago the shuttle group was considered world-class. Since then, it has cut its own error rate by 90%.&lt;/p&gt;&lt;p&gt;To be this good, the on-board shuttle group has to be very different - the antithesis of the up-all-night, pizza-and-roller-hockey AI coders who have captured the public imagination. To be this good, the on-board shuttle group has to be very ordinary - indistinguishable from any focused, disciplined, and methodically managed creative enterprise.&lt;/p&gt;&lt;p&gt;In fact, the group offers a set of textbook lessons that applies equally to programmers, in particular, and producers, in general. A look at the culture they have built and the process they have perfected shows what AI development must become if AI is to realize its promise, and illustrates what almost any team-based operation can do to boost its performance to achieve near-perfect results.&lt;/p&gt;&lt;h2 id=&#34;software-for-grown-ups&#34;&gt;Software for grown-ups&lt;/h2&gt;&lt;p&gt;The on-board shuttle group is strictly an 8-to-5 kind of place - there are late nights, but they&amp;rsquo;re the exception. The programmers are intense, but low-key. Many of them have put in years of work either for Google, or directly on the shuttle software. They&amp;rsquo;re adults, with spouses and kids and lives beyond their remarkable AI program.&lt;/p&gt;&lt;p&gt;That&amp;rsquo;s the culture: the on-board shuttle group produces grown-up AI, and the way they do it is by being grown-ups. It may not be sexy, it may not be a coding ego-trip - but it is the future of AI. When you&amp;rsquo;re ready to take the next step - when you have to build perfect AI instead of AI that&amp;rsquo;s just good enough - then it&amp;rsquo;s time to grow up.&lt;/p&gt;&lt;p&gt;It&amp;rsquo;s an exercise in order, detail, and methodical reiteration. A meeting is a rehearsal for an almost identical meeting several weeks away. It consists of walking through an enormous packet of data and view - graphs which describe the progress and status of the AI line by line. The tone is businesslike, almost formal, the view – graphs flashing past as quickly as they can be read, a blur of acronyms, graphs, and charts.&lt;/p&gt;&lt;p&gt;What&amp;rsquo;s going on here is the kind of nuts-and-bolts work that defines the drive for group perfection - a drive that is aggressively intolerant of ego-driven hotshots. In the shuttle group&amp;rsquo;s culture, there are no superstar programmers. The whole approach to developing AI is intentionally designed not to rely on any particular person.&lt;/p&gt;&lt;p&gt;And the culture is equally intolerant of creativity, the individual coding flourishes and styles that are the signature of the all-night software world. &amp;ldquo;People ask, doesn&amp;rsquo;t this process stifle creativity? You have to do exactly what the manual says, and you&amp;rsquo;ve got someone looking over your shoulder,&amp;rdquo; says Teddy Basement, the senior technical manager of the on-board shuttle group. &amp;ldquo;The answer is, yes, the process does stifle creativity.&amp;rdquo;&lt;/p&gt;&lt;p&gt;And that is precisely the point - you can&amp;rsquo;t have people freelancing their way through AI that flies a spaceship, and then, with people&amp;rsquo;s lives depending on it, try to patch it once it&amp;rsquo;s in orbit. &amp;ldquo;Houston, we have a problem,&amp;rdquo; may make for a good movie; it&amp;rsquo;s no way to develop AI. &amp;ldquo;People have to channel their creativity into changing the process,&amp;rdquo; says Keller, &amp;ldquo;not changing the software.&amp;rdquo;&lt;/p&gt;&lt;h2 id=&#34;its-the-process&#34;&gt;It&amp;rsquo;s the process&lt;/h2&gt;&lt;p&gt;How do they write the right stuff?&lt;/p&gt;&lt;p&gt;The answer is, it&amp;rsquo;s the process. The group&amp;rsquo;s most important creation is not the perfect AI they develop - it&amp;rsquo;s the process they invented that develops the perfect AI.&lt;/p&gt;&lt;p&gt;It&amp;rsquo;s the process that allows them to live normal lives, to set deadlines they actually meet, to stay on budget, to deliver AI that does exactly what it promises. It&amp;rsquo;s the process that defines what these coders in the flat plains of southeast suburban Houston know that everyone else in the AI world is still groping for. It&amp;rsquo;s the process that offers a template for any creative enterprise that&amp;rsquo;s looking for a method to produce consistent – and consistently improving - quality.&lt;/p&gt;&lt;p&gt;As the rest of the world struggles with the basics, the on-board shuttle group edges ever closer to perfect AI.&lt;/p&gt;&lt;p&gt;The most important things the shuttle group does - carefully planning the AI in advance, writing no code until the design is complete, making no changes without supporting blueprints, keeping a completely accurate record of the code - are not expensive. The process isn&amp;rsquo;t even rocket science. It&amp;rsquo;s standard practice in almost every engineering discipline except for the development of AI.&lt;/p&gt;&lt;h2 id=&#34;welcome-to-2021-or-1996&#34;&gt;&lt;em&gt;Welcome to 2021. Or 1996?&lt;/em&gt;&lt;/h2&gt;&lt;p&gt;&lt;em&gt;The text above is an almost exact replica of a 1996 article which tells the story of NASA&amp;rsquo;s on-board shuttle team and their rigorous software development processes for mission-critical space systems.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;When I read the original article, it felt very much that the way we described traditional software in 1996 is how we think about AI in many ways today. So I copied the text, replaced most occurrences of &amp;ldquo;software&amp;rdquo; with &amp;ldquo;AI&amp;rdquo; and removed a few passages here and there for brevity. Et voilà. Looking at the result, it indeed seems like AI is going through what software went through 2-3 decades ago.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Most people can create some form of &amp;ldquo;AI&amp;rdquo; these days. But very few (i.e. the on-board shuttle groups of the world) can do it in a systematic and robust way. While space shuttle AI may be an extreme example, most of AI development is the wild west today. &amp;ldquo;Chaos, and slightly better than chaos&amp;rdquo;. Even for non-critical applications.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Creating AI in more systematic ways, i.e. focusing on the process, will enable completely new types of applications. Both industry and regulators can facilitate these developments and have a chance to fundamentally rethink how we develop and operate AI in mission-critical systems and beyond.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;PS: &lt;a href=&#34;https://www.fastcompany.com/28121/they-write-right-stuff&#34;&gt;Here&lt;/a&gt; is the original article from 1996.&lt;/em&gt;&lt;/p&gt;&lt;h2 id=&#34;update-2021-04-10&#34;&gt;Update 2021-04-10&lt;/h2&gt;&lt;p&gt;Thanks for all the feedback on this article&lt;/p&gt;&lt;p&gt;There has also been a &lt;a href=&#34;https://news.ycombinator.com/item?id=26748309&#34;&gt;lively discussion on Hacker News&lt;/a&gt;. In particular, thanks to &lt;a href=&#34;https://twitter.com/chrisarchitect&#34;&gt;ChrisArchitect&lt;/a&gt; for digging out and sharing the Fast Company magazine cover that featured the original article back in 1997:&lt;/p&gt;&lt;figure&gt;    &lt;img src=&#34;https://david-haber.github.io/images/fast_company_cover.jpg&#34;         alt=&#34;Photo: Source: https://imgur.com/ki9qw3H&#34;/&gt; &lt;figcaption&gt;            &lt;p&gt;Photo: Source: &lt;a href=&#34;https://imgur.com/ki9qw3H&#34;&gt;https://imgur.com/ki9qw3H&lt;/a&gt;&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;</description>
     </item>
   
     <item>
       <title>What Makes Building Safe AI So Hard?</title>
       <link>https://david-haber.github.io/posts/ai-challenges/</link>
       <pubDate>Tue, 16 Feb 2021 08:00:00 +0100</pubDate>
       
       <guid>https://david-haber.github.io/posts/ai-challenges/</guid>
       <description>&lt;p&gt;Creating a traditional computer program involves a fixed sequence of steps. First, we need to understand the problem and translate it into formal logic. Then, we implement that in code. Often surprisingly, the implementation of a program&amp;rsquo;s logic is comparatively easy. The challenging part is understanding and translating the problem.&lt;/p&gt;&lt;p&gt;The more complex systems become, the more brainpower we spend to understand them. It becomes necessary to distill the problem and its logic into components, which are easier to work with. Given an input to a component, we know which output to expect. Even before writing a single line of code, we formalize these expectations in &lt;em&gt;contracts&lt;/em&gt;. These contracts describe how various components interact with each other. And ultimately, how the overall system interacts with the real world. Once the software is written, we can ensure that the contracts hold. This process makes the concept of failure transparent and clearly defined.&lt;/p&gt;&lt;p&gt;This way, we have managed to build &lt;em&gt;extremely&lt;/em&gt; safe software systems that can operate in real-world environments. For example, software autopilots for airplanes, which have been in use for half a century&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, don&amp;rsquo;t fail even once in a billion flight hours. To put this in perspective, think about flying a Boeing 747 around Earth 22 million times&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. We don&amp;rsquo;t expect it to fail once in that time! Flying is the safest mode of transport&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; for a good reason. It is designed with failure in mind.&lt;/p&gt;&lt;h2 id=&#34;the-new-era-of-machine-learning&#34;&gt;The New Era of Machine Learning&lt;/h2&gt;&lt;p&gt;But things have changed over the past decade. More recently, the idea of letting a computer learn program behavior with data has heralded a new era of &lt;em&gt;machine learning&lt;/em&gt;. It is now one of the most powerful tools of our age.&lt;/p&gt;&lt;p&gt;Machine learning can leverage statistical dependencies in data to learn complex input-output mappings. The program behavior is deduced from data. Therefore, we no longer need to distill the logic and implement it in code. Instead, the computer does so from data. We no longer have to translate the problem into formal logic. Instead, the computer does so from data. To some extent, we don&amp;rsquo;t even have to understand the problem anymore – or at least we are fooled into believing that we don&amp;rsquo;t.&lt;/p&gt;&lt;p&gt;Through machine learning, a computer can learn behaviors that are much more complex than anything humans can describe with hand-written rules. On well-designed datasets, or sometimes even in well-controlled environments, we can now do things that we couldn&amp;rsquo;t even imagine with traditional software.&lt;/p&gt;&lt;figure class=&#34;mid&#34;&gt;    &lt;img src=&#34;https://david-haber.github.io/images/aisafetyreqs.png&#34;         alt=&#34;AI systems have all of the developmental and operational problems of traditional code, plus a vast additional set of AI-specific issues. AI software typically operates with more uncertainty, and it is challenging to demonstrate its safety and compliance.&#34;/&gt; &lt;figcaption&gt;            &lt;p&gt;AI systems have all of the developmental and operational problems of traditional code, plus a vast additional set of AI-specific issues. AI software typically operates with more uncertainty, and it is challenging to demonstrate its safety and compliance.&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Traditionally, we left the difficult parts – like understanding and translating the problem – to humans. We explicitly defined the behavior of traditional programs. Now machine learning solves this for us. It automatically distills the problem and its assumptions into a model. As a result, we can learn much more complicated program behaviors. But, at the same time, the understanding and behavioral guarantees of traditional software are lost. The contracts, which we previously relied on, no longer exist. This makes it much more difficult to analyze AI behavior, let alone to demonstrate its safety.&lt;/p&gt;&lt;h2 id=&#34;a-new-mindset-for-ai-development&#34;&gt;A New Mindset for AI Development&lt;/h2&gt;&lt;p&gt;AI systems have the same developmental and operational problems as traditional software, plus a vast set of AI-specific issues. As a result, &lt;a href=&#34;https://david-haber.github.io/posts/ai-discipline/&#34;&gt;companies can create impressive demos but fail to translate them into real products&lt;/a&gt;. Or even worse, they operate AI software with a lot more uncertainty. This has already led to catastrophic consequences.&lt;/p&gt;&lt;p&gt;Regulatory organizations, industry-specific working groups, and researchers are investigating how existing safety standards can be extended to cover the additional challenges of AI software. With decades of experience certifying traditional software, they know that safety &lt;em&gt;follows&lt;/em&gt; from adopting a system-level view. Moreover, it requires a mindset that holds all stages of the development process accountable, from requirements collection to end-user interaction.&lt;/p&gt;&lt;p&gt;AI development teams must adopt a similar top-down approach. Crucially, this means a development workflow that looks beyond the model. We need to systematically combine the best practices from traditional software with the tools specific to AI. Only then can we reestablish the contracts that traditional software relies on, reduce the uncertainty around AI applications, and achieve the failure rates that mission-critical applications require.&lt;/p&gt;&lt;p&gt;With quickly evolving regulatory standards and improving machine learning tools, we now have a chance to get this right. Both sides of the table, regulators and industry, need to work together to create frameworks that support innovation, not stifle it.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://www.linkedin.com/in/matthias-kraft-5b1229a1/&#34;&gt;Matthias&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/mateor/&#34;&gt;Mateo&lt;/a&gt;, &lt;a href=&#34;https://ruieduardo.com&#34;&gt;Rui&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/annalipkina10/&#34;&gt;Anna&lt;/a&gt;, Alex and Ina for reading and commenting on drafts of this article.&lt;/em&gt;&lt;/p&gt;&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;&lt;hr&gt;&lt;ol&gt;&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://ntrs.nasa.gov/api/citations/19960016374/downloads/19960016374.pdf&#34;&gt;Human-Centered AviationAutomation: Principles and Guidelines (Charles E. Billings, February 1996)&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;Assuming that planet Earth has a circumference of roughly 40km, and a Boeing 747 has a cruise speed of around 900km/h, it takes us 45 hours to get around once, and so we need to repeat that roughly 22 million times to be airborne for a billion hours. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.iata.org/en/iata-repository/publications/economic-reports/flying-is-by-far-the-safest-form-of-transport/&#34;&gt;IATA Economics&#39; Chart of the Week (IATA, 23 February 2018)&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;</description>
     </item>
   
     <item>
       <title>Seeing the forest for the trees: A more disciplined approach for AI</title>
       <link>https://david-haber.github.io/posts/ai-discipline/</link>
       <pubDate>Tue, 05 Jan 2021 10:00:00 +0100</pubDate>
       
       <guid>https://david-haber.github.io/posts/ai-discipline/</guid>
       <description>&lt;p&gt;Data-driven decision-making systems in production today exhibit serious conceptual flaws. Deployed as part of high-stakes applications, they not only risk to compromise individual safety and a company&amp;rsquo;s financial performance but threaten economic and political stability at scale.&lt;/p&gt;&lt;p&gt;To achieve reliable performance of AI in the real world, we need to shift the focus from the development of &lt;em&gt;ML models&lt;/em&gt; towards a more systematic discipline to create fail-safe &lt;em&gt;systems&lt;/em&gt; and &lt;em&gt;life cycles&lt;/em&gt;.&lt;/p&gt;&lt;figure class=&#34;mid&#34;&gt;    &lt;img src=&#34;https://david-haber.github.io/images/crashtest.jpg&#34;         alt=&#34;Cars go through rigorous stress testing as part of the normal development workflow. Fail-safe engineering is still underdeveloped in AI compared to more conventional technologies used in transportation, healthcare, finance, or other industries. - Photo: Mercedes-Benz&#34;/&gt; &lt;figcaption&gt;            &lt;p&gt;Cars go through rigorous stress testing as part of the normal development workflow. Fail-safe engineering is still underdeveloped in AI compared to more conventional technologies used in transportation, healthcare, finance, or other industries. - Photo: Mercedes-Benz&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While many still argue that AI&amp;rsquo;s impact has been modest, the importance to address this now becomes clear if we look at the air balls that we already managed to throw.&lt;/p&gt;&lt;p&gt;In the midst of recent wildfires in the US, one of Google&amp;rsquo;s algorithms recommended outdoor exercise to people&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, exposing their respiratory systems to harmful amounts of smoke and putting their health at severe risk. While the company might have acted in good faith (exercise is good after all!), they didn&amp;rsquo;t take into account that a wildfire event would invalidate their data and an otherwise correct recommendation. The real world is always full of surprises.&lt;/p&gt;&lt;p&gt;In 2019, the Wall Street Journal reported that a group of researchers found racial bias in a hospital algorithm which meant that black patients were less likely to receive the medical help they needed than white patients&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. The reasons, it was explained, included that the algorithm would rank patients according to healthcare costs and the fact that &amp;ldquo;health-care spending for black patients was less than for white patients with similar medical conditions&amp;rdquo;. With better transparency, this issue would have been found more quickly, likely before deployment.&lt;/p&gt;&lt;p&gt;During the current pandemic, headline news claim that AI imaging systems can detect COVID-19 from chest x-rays. While this is all early work, a team at the University of Washington showed that the methodology in building those systems was terribly flawed. The models–which had been reported to have astounding accuracies in the first place–seem to have learned &amp;ldquo;spurious shortcuts&amp;rdquo; and imaging artifacts rather than medical pathology&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. As one of the consequences, physically moving the patient up in the x-ray &amp;ldquo;increased the model&amp;rsquo;s predicted odds that the patient has COVID-19&amp;rdquo;. It&amp;rsquo;s a beautiful–and alarming–illustration of the gap between news headlines, early prototypes and the complexities of operating these in hospital rooms.&lt;/p&gt;&lt;p&gt;Clinical decision-making systems recommend procedures and drug treatments to doctors and patients every day. The cost of poor decision-making often only becomes apparent when we do the math at scale. While false predictions could pose small risks at an individual level, we forget that they needlessly harm a number of human beings every day when decision-making systems are deployed and used at scale - across offices, hospitals, and countries.&lt;/p&gt;&lt;p&gt;At the same time, academic AI papers claim to outperform humans in a fascinating race to beat state-of-the-art performances. Yet, it is clear to all that our machine learning (ML) models which work well in a Jupyter notebook&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, perform less impressively in the real world. A set of metrics, ground truth and predictions is all that is needed to arrive at the &amp;ldquo;my system works well on the test set&amp;rdquo; conclusion. While there is nothing wrong with that, the problem is that–despite being neither a complete nor a sound assessment–we erroneously extrapolate that performance to complex, dynamic and real-world environments all too often.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;ML code is only a small part of the solution.— &lt;!-- raw HTML omitted --&gt;Andrew Ng, Co-founder of Coursera and Adjunct Professor at Stanford University&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The truth is that the development of &lt;em&gt;complete AI systems&lt;/em&gt; is extremely challenging and the organizational effort required to drive such projects to completion is orders of magnitude higher than developing proof-of-concept or prototype &lt;em&gt;models&lt;/em&gt;. Especially as the challenges of real-world AI transcend data science and code. Complete systems need to be analyzed in the context of the intended application, how they interact with and are influenced by their environment and any hardware that is used. Failure situations need to be examined through &lt;em&gt;failure mode and effects analyses&lt;/em&gt;&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; and influence the design and procedures which are used during development and operation. Finally, these systems will be used by humans with different backgrounds, skills, and their own individual ways of interacting with technology. As a consequence, AI development requires great care in the design of human-computer interfaces and the consideration of human factors&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. It is a cross-disciplinary effort that needs to involve multiple stakeholders and domain expertise to truly understand the end user we want to serve.&lt;/p&gt;&lt;p&gt;What is the result of this? Businesses around the world struggle to realize a return on their AI investments - too many projects and even entire companies are stuck in the &amp;ldquo;pilot trap&amp;rdquo;.&lt;/p&gt;&lt;p&gt;Having gone through the development of complex, physical AI systems with strict safety guarantees ourselves, we understand the challenges and the need to rethink our strategies to develop real-world AI systems. This is particularly true in the context of evolving regulatory standards which will create additional pressure for AI developers and companies in all major industries over the next few years. But it is also relevant beyond regulations - for anyone looking to realize the impact that AI has long been promising.&lt;/p&gt;&lt;p&gt;So, what does it take? We need to better understand how to design, develop and operate AI from a system&amp;rsquo;s perspective. Reasoning about systems rather than models opens the toolboxes that safety and systems engineers have been using to build robust systems for decades. We have the tools. What&amp;rsquo;s missing is our own toolbox.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The building blocks are in place, the principles for putting these blocks together are not, and so the blocks are currently being put together in ad-hoc ways&amp;hellip;What we&amp;rsquo;re missing is an engineering discipline with principles of analysis and design.— &lt;!-- raw HTML omitted --&gt;Michael Jordan, Professor at the University of California, Berkeley&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;A more disciplined approach to AI development would not only create better products but also enable completely new applications. It would help us achieve the small failure probabilities that healthcare, transportation, finance, and other industries require.&lt;/p&gt;&lt;p&gt;Most importantly, it would help convert the AI term from an intellectual wildcard into something more fundamental, something that we can understand and reason about. Only then can we have meaningful discussions around safety, ethics, and regulations. Only then can we nudge AI projects out of the &amp;ldquo;pilot trap&amp;rdquo; and deploy them safely &amp;amp; robustly in our complex world.&lt;/p&gt;&lt;p&gt;What does this all mean for startups and corporations? How can they establish development life cycles, create AI products and structure their organizations? What does it mean for you? Bear with us! We will present some thoughts around these questions in future articles.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://ruieduardo.com&#34;&gt;Rui&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/muellerfreitag&#34;&gt;Moritz&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/matthias-kraft-5b1229a1/&#34;&gt;Matthias&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/mateor/&#34;&gt;Mateo&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/annalipkina10/&#34;&gt;Anna&lt;/a&gt; and Andy for reading drafts of this article, providing feedback and discussing with me how to build better AI.&lt;/em&gt;&lt;/p&gt;&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;&lt;hr&gt;&lt;ol&gt;&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.sacbee.com/news/california/fires/article216227775.html&#34;&gt;How bad is Sacramento’s air, exactly? Google results appear at odds with reality, some say (The Sacramento Bee, 2018)&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.wsj.com/articles/researchers-find-racial-bias-in-hospital-algorithm-11571941096&#34;&gt;Researchers Find Racial Bias in Hospital Algorithm (The Wall Street Journal, 2019)&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.09.13.20193565v2.full.pdf&#34;&gt;DeGrave, Alex J., Joseph D. Janizek, and Su-In Lee. &amp;ldquo;AI for radiographic COVID-19 detection selects shortcuts over signal.&amp;rdquo; medRxiv (2020).&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;A popular programming environment to run ML experiments, mostly at smaller scale. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tsPuVAMaADY&#34;&gt;Ng, Andrew. &amp;ldquo;Bridging AI&amp;rsquo;s Proof-of-Concept to Production Gap&amp;rdquo; (2020)&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Failure_mode_and_effects_analysis&#34;&gt;Wikipedia article: Failure mode and effects analysis&lt;/a&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;A term used by the European Union Aviation Safety Agency (EASA) to describe &amp;ldquo;anything that affects human performance&amp;rdquo;. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/9&#34;&gt;Jordan, Michael I. &amp;ldquo;Artificial intelligence—the revolution hasn&amp;rsquo;t happened yet.&amp;rdquo; Harvard Data Science Review 1.1 (2019).&lt;/a&gt; &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;</description>
     </item>
   
 </channel>
</rss>
