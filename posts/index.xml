
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
 <channel>
   <title>Posts on David Haber</title>
   <link>https://david-haber.github.io/posts/</link>
   <description>Recent content in Posts on David Haber</description>
   <generator>Hugo -- gohugo.io</generator>
   <language>en-us</language>
   <copyright>Copyright &amp;copy; 2021 - David Haber</copyright>
   <lastBuildDate>Tue, 16 Feb 2021 08:00:00 +0100</lastBuildDate>
   
       <atom:link href="https://david-haber.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
   
   
     <item>
       <title>What Makes Building Safe AI So Hard?</title>
       <link>https://david-haber.github.io/posts/ai-challenges/</link>
       <pubDate>Tue, 16 Feb 2021 08:00:00 +0100</pubDate>
       
       <guid>https://david-haber.github.io/posts/ai-challenges/</guid>
       <description>&lt;p&gt;Creating a traditional computer program involves a fixed sequence of steps. First, we need to understand the problem and translate it into formal logic. Then, we implement that in code. Often surprisingly, the implementation of a program&amp;rsquo;s logic is comparatively easy. The challenging part is understanding and translating the problem.&lt;/p&gt;&lt;p&gt;The more complex systems become, the more brainpower we spend to understand them. It becomes necessary to distill the problem and its logic into components, which are easier to work with. Given an input to a component, we know which output to expect. Even before writing a single line of code, we formalize these expectations in &lt;em&gt;contracts&lt;/em&gt;. These contracts describe how various components interact with each other. And ultimately, how the overall system interacts with the real world. Once the software is written, we can ensure that the contracts hold. This process makes the concept of failure transparent and clearly defined.&lt;/p&gt;&lt;p&gt;This way, we have managed to build &lt;em&gt;extremely&lt;/em&gt; safe software systems that can operate in real-world environments. For example, software autopilots for airplanes, which have been in use for half a century&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, don&amp;rsquo;t fail even once in a billion flight hours. To put this in perspective, think about flying a Boeing 747 around Earth 22 million times&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. We don&amp;rsquo;t expect it to fail once in that time! Flying is the safest mode of transport&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; for a good reason. It is designed with failure in mind.&lt;/p&gt;&lt;h2 id=&#34;the-new-era-of-machine-learning&#34;&gt;The New Era of Machine Learning&lt;/h2&gt;&lt;p&gt;But things have changed over the past decade. More recently, the idea of letting a computer learn program behavior with data has heralded a new era of &lt;em&gt;machine learning&lt;/em&gt;. It is now one of the most powerful tools of our age.&lt;/p&gt;&lt;p&gt;Machine learning can leverage statistical dependencies in data to learn complex input-output mappings. The program behavior is deduced from data. Therefore, we no longer need to distill the logic and implement it in code. Instead, the computer does so from data. We no longer have to translate the problem into formal logic. Instead, the computer does so from data. To some extent, we don&amp;rsquo;t even have to understand the problem anymore – or at least we are fooled into believing that we don&amp;rsquo;t.&lt;/p&gt;&lt;p&gt;Through machine learning, a computer can learn behaviors that are much more complex than anything humans can describe with hand-written rules. On well-designed datasets, or sometimes even in well-controlled environments, we can now do things that we couldn&amp;rsquo;t even imagine with traditional software.&lt;/p&gt;&lt;figure class=&#34;mid&#34;&gt;    &lt;img src=&#34;https://david-haber.github.io/images/aisafetyreqs.png&#34;         alt=&#34;AI systems have all of the developmental and operational problems of traditional code, plus a vast additional set of AI-specific issues. AI software typically operates with more uncertainty, and it is challenging to demonstrate its safety and compliance.&#34;/&gt; &lt;figcaption&gt;            &lt;p&gt;AI systems have all of the developmental and operational problems of traditional code, plus a vast additional set of AI-specific issues. AI software typically operates with more uncertainty, and it is challenging to demonstrate its safety and compliance.&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Traditionally, we left the difficult parts – like understanding and translating the problem – to humans. We explicitly defined the behavior of traditional programs. Now machine learning solves this for us. It automatically distills the problem and its assumptions into a model. As a result, we can learn much more complicated program behaviors. But, at the same time, the understanding and behavioral guarantees of traditional software are lost. The contracts, which we previously relied on, no longer exist. This makes it much more difficult to analyze AI behavior, let alone to demonstrate its safety.&lt;/p&gt;&lt;h2 id=&#34;a-new-mindset-for-ai-development&#34;&gt;A New Mindset for AI Development&lt;/h2&gt;&lt;p&gt;AI systems have the same developmental and operational problems as traditional software, plus a vast set of AI-specific issues. As a result, &lt;a href=&#34;https://david-haber.github.io/posts/ai-discipline/&#34;&gt;companies can create impressive demos but fail to translate them into real products&lt;/a&gt;. Or even worse, they operate AI software with a lot more uncertainty. This has already led to catastrophic consequences.&lt;/p&gt;&lt;p&gt;Regulatory organizations, industry-specific working groups, and researchers are investigating how existing safety standards can be extended to cover the additional challenges of AI software. With decades of experience certifying traditional software, they know that safety &lt;em&gt;follows&lt;/em&gt; from adopting a system-level view. Moreover, it requires a mindset that holds all stages of the development process accountable, from requirements collection to end-user interaction.&lt;/p&gt;&lt;p&gt;AI development teams must adopt a similar top-down approach. Crucially, this means a development workflow that looks beyond the model. We need to systematically combine the best practices from traditional software with the tools specific to AI. Only then can we reestablish the contracts that traditional software relies on, reduce the uncertainty around AI applications, and achieve the failure rates that mission-critical applications require.&lt;/p&gt;&lt;p&gt;With quickly evolving regulatory standards and improving machine learning tools, we now have a chance to get this right. Both sides of the table, regulators and industry, need to work together to create frameworks that support innovation, not stifle it.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://www.linkedin.com/in/matthias-kraft-5b1229a1/&#34;&gt;Matthias&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/mateor/&#34;&gt;Mateo&lt;/a&gt;, &lt;a href=&#34;https://ruieduardo.com&#34;&gt;Rui&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/annalipkina10/&#34;&gt;Anna&lt;/a&gt;, Alex and Ina for reading and commenting on drafts of this article.&lt;/em&gt;&lt;/p&gt;&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;&lt;hr&gt;&lt;ol&gt;&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://ntrs.nasa.gov/api/citations/19960016374/downloads/19960016374.pdf&#34;&gt;Human-Centered AviationAutomation: Principles and Guidelines (Charles E. Billings, February 1996)&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;Assuming that planet Earth has a circumference of roughly 40km, and a Boeing 747 has a cruise speed of around 900km/h, it takes us 45 hours to get around once, and so we need to repeat that roughly 22 million times to be airborne for a billion hours. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.iata.org/en/iata-repository/publications/economic-reports/flying-is-by-far-the-safest-form-of-transport/&#34;&gt;IATA Economics&#39; Chart of the Week (IATA, 23 February 2018)&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;</description>
     </item>
   
     <item>
       <title>Seeing the forest for the trees: A more disciplined approach for AI</title>
       <link>https://david-haber.github.io/posts/ai-discipline/</link>
       <pubDate>Tue, 05 Jan 2021 10:00:00 +0100</pubDate>
       
       <guid>https://david-haber.github.io/posts/ai-discipline/</guid>
       <description>&lt;p&gt;Data-driven decision-making systems in production today exhibit serious conceptual flaws. Deployed as part of high-stakes applications, they not only risk to compromise individual safety and a company&amp;rsquo;s financial performance but threaten economic and political stability at scale.&lt;/p&gt;&lt;p&gt;To achieve reliable performance of AI in the real world, we need to shift the focus from the development of &lt;em&gt;ML models&lt;/em&gt; towards a more systematic discipline to create fail-safe &lt;em&gt;systems&lt;/em&gt; and &lt;em&gt;life cycles&lt;/em&gt;.&lt;/p&gt;&lt;figure class=&#34;mid&#34;&gt;    &lt;img src=&#34;https://david-haber.github.io/images/crashtest.jpg&#34;         alt=&#34;Cars go through rigorous stress testing as part of the normal development workflow. Fail-safe engineering is still underdeveloped in AI compared to more conventional technologies used in transportation, healthcare, finance, or other industries. - Photo: Mercedes-Benz&#34;/&gt; &lt;figcaption&gt;            &lt;p&gt;Cars go through rigorous stress testing as part of the normal development workflow. Fail-safe engineering is still underdeveloped in AI compared to more conventional technologies used in transportation, healthcare, finance, or other industries. - Photo: Mercedes-Benz&lt;/p&gt;        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While many still argue that AI&amp;rsquo;s impact has been modest, the importance to address this now becomes clear if we look at the air balls that we already managed to throw.&lt;/p&gt;&lt;p&gt;In the midst of recent wildfires in the US, one of Google&amp;rsquo;s algorithms recommended outdoor exercise to people&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, exposing their respiratory systems to harmful amounts of smoke and putting their health at severe risk. While the company might have acted in good faith (exercise is good after all!), they didn&amp;rsquo;t take into account that a wildfire event would invalidate their data and an otherwise correct recommendation. The real world is always full of surprises.&lt;/p&gt;&lt;p&gt;In 2019, the Wall Street Journal reported that a group of researchers found racial bias in a hospital algorithm which meant that black patients were less likely to receive the medical help they needed than white patients&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. The reasons, it was explained, included that the algorithm would rank patients according to healthcare costs and the fact that &amp;ldquo;health-care spending for black patients was less than for white patients with similar medical conditions&amp;rdquo;. With better transparency, this issue would have been found more quickly, likely before deployment.&lt;/p&gt;&lt;p&gt;During the current pandemic, headline news claim that AI imaging systems can detect COVID-19 from chest x-rays. While this is all early work, a team at the University of Washington showed that the methodology in building those systems was terribly flawed. The models–which had been reported to have astounding accuracies in the first place–seem to have learned &amp;ldquo;spurious shortcuts&amp;rdquo; and imaging artifacts rather than medical pathology&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;. As one of the consequences, physically moving the patient up in the x-ray &amp;ldquo;increased the model&amp;rsquo;s predicted odds that the patient has COVID-19&amp;rdquo;. It&amp;rsquo;s a beautiful–and alarming–illustration of the gap between news headlines, early prototypes and the complexities of operating these in hospital rooms.&lt;/p&gt;&lt;p&gt;Clinical decision-making systems recommend procedures and drug treatments to doctors and patients every day. The cost of poor decision-making often only becomes apparent when we do the math at scale. While false predictions could pose small risks at an individual level, we forget that they needlessly harm a number of human beings every day when decision-making systems are deployed and used at scale - across offices, hospitals, and countries.&lt;/p&gt;&lt;p&gt;At the same time, academic AI papers claim to outperform humans in a fascinating race to beat state-of-the-art performances. Yet, it is clear to all that our machine learning (ML) models which work well in a Jupyter notebook&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, perform less impressively in the real world. A set of metrics, ground truth and predictions is all that is needed to arrive at the &amp;ldquo;my system works well on the test set&amp;rdquo; conclusion. While there is nothing wrong with that, the problem is that–despite being neither a complete nor a sound assessment–we erroneously extrapolate that performance to complex, dynamic and real-world environments all too often.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;ML code is only a small part of the solution.— &lt;!-- raw HTML omitted --&gt;Andrew Ng, Co-founder of Coursera and Adjunct Professor at Stanford University&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The truth is that the development of &lt;em&gt;complete AI systems&lt;/em&gt; is extremely challenging and the organizational effort required to drive such projects to completion is orders of magnitude higher than developing proof-of-concept or prototype &lt;em&gt;models&lt;/em&gt;. Especially as the challenges of real-world AI transcend data science and code. Complete systems need to be analyzed in the context of the intended application, how they interact with and are influenced by their environment and any hardware that is used. Failure situations need to be examined through &lt;em&gt;failure mode and effects analyses&lt;/em&gt;&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; and influence the design and procedures which are used during development and operation. Finally, these systems will be used by humans with different backgrounds, skills, and their own individual ways of interacting with technology. As a consequence, AI development requires great care in the design of human-computer interfaces and the consideration of human factors&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. It is a cross-disciplinary effort that needs to involve multiple stakeholders and domain expertise to truly understand the end user we want to serve.&lt;/p&gt;&lt;p&gt;What is the result of this? Businesses around the world struggle to realize a return on their AI investments - too many projects and even entire companies are stuck in the &amp;ldquo;pilot trap&amp;rdquo;.&lt;/p&gt;&lt;p&gt;Having gone through the development of complex, physical AI systems with strict safety guarantees ourselves, we understand the challenges and the need to rethink our strategies to develop real-world AI systems. This is particularly true in the context of evolving regulatory standards which will create additional pressure for AI developers and companies in all major industries over the next few years. But it is also relevant beyond regulations - for anyone looking to realize the impact that AI has long been promising.&lt;/p&gt;&lt;p&gt;So, what does it take? We need to better understand how to design, develop and operate AI from a system&amp;rsquo;s perspective. Reasoning about systems rather than models opens the toolboxes that safety and systems engineers have been using to build robust systems for decades. We have the tools. What&amp;rsquo;s missing is our own toolbox.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The building blocks are in place, the principles for putting these blocks together are not, and so the blocks are currently being put together in ad-hoc ways&amp;hellip;What we&amp;rsquo;re missing is an engineering discipline with principles of analysis and design.— &lt;!-- raw HTML omitted --&gt;Michael Jordan, Professor at the University of California, Berkeley&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;A more disciplined approach to AI development would not only create better products but also enable completely new applications. It would help us achieve the small failure probabilities that healthcare, transportation, finance, and other industries require.&lt;/p&gt;&lt;p&gt;Most importantly, it would help convert the AI term from an intellectual wildcard into something more fundamental, something that we can understand and reason about. Only then can we have meaningful discussions around safety, ethics, and regulations. Only then can we nudge AI projects out of the &amp;ldquo;pilot trap&amp;rdquo; and deploy them safely &amp;amp; robustly in our complex world.&lt;/p&gt;&lt;p&gt;What does this all mean for startups and corporations? How can they establish development life cycles, create AI products and structure their organizations? What does it mean for you? Bear with us! We will present some thoughts around these questions in future articles.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://ruieduardo.com&#34;&gt;Rui&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/muellerfreitag&#34;&gt;Moritz&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/matthias-kraft-5b1229a1/&#34;&gt;Matthias&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/mateor/&#34;&gt;Mateo&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/annalipkina10/&#34;&gt;Anna&lt;/a&gt; and Andy for reading drafts of this article, providing feedback and discussing with me how to build better AI.&lt;/em&gt;&lt;/p&gt;&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;&lt;hr&gt;&lt;ol&gt;&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.sacbee.com/news/california/fires/article216227775.html&#34;&gt;How bad is Sacramento’s air, exactly? Google results appear at odds with reality, some say (The Sacramento Bee, 2018)&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.wsj.com/articles/researchers-find-racial-bias-in-hospital-algorithm-11571941096&#34;&gt;Researchers Find Racial Bias in Hospital Algorithm (The Wall Street Journal, 2019)&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.09.13.20193565v2.full.pdf&#34;&gt;DeGrave, Alex J., Joseph D. Janizek, and Su-In Lee. &amp;ldquo;AI for radiographic COVID-19 detection selects shortcuts over signal.&amp;rdquo; medRxiv (2020).&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;A popular programming environment to run ML experiments, mostly at smaller scale. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tsPuVAMaADY&#34;&gt;Ng, Andrew. &amp;ldquo;Bridging AI&amp;rsquo;s Proof-of-Concept to Production Gap&amp;rdquo; (2020)&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Failure_mode_and_effects_analysis&#34;&gt;Wikipedia article: Failure mode and effects analysis&lt;/a&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;A term used by the European Union Aviation Safety Agency (EASA) to describe &amp;ldquo;anything that affects human performance&amp;rdquo;. &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/9&#34;&gt;Jordan, Michael I. &amp;ldquo;Artificial intelligence—the revolution hasn&amp;rsquo;t happened yet.&amp;rdquo; Harvard Data Science Review 1.1 (2019).&lt;/a&gt; &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/section&gt;</description>
     </item>
   
 </channel>
</rss>
